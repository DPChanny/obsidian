### **1. 정보량 (Information Content)**

어떤 사건 x가 발생했다는 것을 알게 되었을 때 얻는 정보의 양을 **정보량**이라고 합니다. 확률이 낮은 사건일수록 더 많은 정보를 담고 있습니다. 예를 들어, "해가 동쪽에서 떴다" (확률 ≈ 1)는 정보량이 거의 없지만, "4년마다 열리는 월드컵에서 한국이 우승했다" (확률 ≪ 1)는 엄청난 정보량을 가집니다.

사건 x의 정보량 $I(x)$는 확률 $P(x)$에 반비례하며, 다음과 같이 로그 함수를 이용해 정의됩니다. 이는 독립적인 사건들의 정보량을 더할 수 있도록 하기 위함입니다.

I(x)=−log(P(x))

여기서 로그의 밑은 보통 2(정보 단위를 '비트'로 사용), 자연상수 e(자연로그), 또는 10을 사용합니다. 머신러닝에서는 주로 자연로그를 사용합니다.

두 확률의 사건에 대해서 정보량은 더해지는 것이 자연스러움으로 로그를 활용

---

### **2. 엔트로피 (Entropy)**

**엔트로피(Entropy)**는 어떤 확률분포 P가 갖는 불확실성(uncertainty)의 기댓값, 또는 사건들의 평균 정보량을 의미합니다. 즉, 해당 확률분포에서 하나의 사건을 샘플링했을 때, 우리가 얻을 것으로 기대되는 평균 정보량입니다.

확률분포 P에 대한 엔트로피 $H(P)$는 각 사건의 정보량에 해당 사건이 일어날 확률을 곱한 뒤 모두 더하여 계산합니다.

H(P)=x∑​P(x)I(x)=−x∑​P(x)log(P(x))

엔트로피가 높다는 것은 확률분포의 불확실성이 크다는 것을 의미합니다. 예를 들어, 모든 사건이 동일한 확률로 발생하는 균등분포(uniform distribution)에서 엔트로피는 최댓값을 가집니다. 반면, 특정 사건의 확률이 1이고 나머지가 0인 분포는 불확실성이 없으므로 엔트로피가 0입니다.

---

### **3. 크로스 엔트로피 (Cross-Entropy)**

**크로스 엔트로피**는 동일한 사건 공간에 대한 두 확률분포 P와 Q 사이의 차이를 측정하는 방법 중 하나입니다. 여기서 P는 실제(참) 확률분포(true distribution)를, Q는 모델이 예측한 확률분포(predicted distribution)를 나타냅니다.

크로스 엔트로피 $H(P, Q)$는 실제 분포 P의 관점에서, 모델이 예측한 분포 Q를 사용하여 정보량을 계산했을 때의 평균 정보량을 의미합니다. 즉, **실제 분포 P의 사건이 발생했을 때, 이를 예측 분포 Q의 확률로 인코딩하는 데 필요한 평균 비트(정보량) 수**를 나타냅니다.

수식은 다음과 같습니다.

H(P,Q)=−x∑​P(x)log(Q(x))

- P(x): 실제 분포에서 사건 x가 발생할 확률
    
- Q(x): 예측 분포에서 사건 x가 발생할 확률
    

만약 예측 분포 Q가 실제 분포 P와 완벽하게 동일하다면 (Q=P), 크로스 엔트로피는 엔트로피와 같아집니다 (H(P,P)=H(P)).

---

### **4. 쿨백-라이블러 발산 (KL Divergence)과의 관계**

크로스 엔트로피는 **쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL Divergence)**과 밀접한 관련이 있습니다. KL 발산은 두 확률분포 P와 Q가 얼마나 다른지를 측정하는 지표이며, 다음과 같이 정의됩니다.

DKL​(P∣∣Q)=x∑​P(x)log(Q(x)P(x)​)

이 식을 전개하면 크로스 엔트로피와 엔트로피의 관계로 표현할 수 있습니다.

DKL​(P∣∣Q)​=x∑​P(x)(log(P(x))−log(Q(x)))=x∑​P(x)log(P(x))−x∑​P(x)log(Q(x))=−(−x∑​P(x)log(P(x)))+(−x∑​P(x)log(Q(x)))=−H(P)+H(P,Q)​

따라서 크로스 엔트로피는 다음과 같이 재정의될 수 있습니다.

H(P,Q)=H(P)+DKL​(P∣∣Q)