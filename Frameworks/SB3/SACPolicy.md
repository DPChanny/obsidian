---
---
1. 내부 신경망 구축 (__init__)
SACPolicy가 생성될 때, 다음과 같은 주요 신경망들이 순차적으로 구축된다.
 * 특징 추출기 (features_extractor) 생성
   * 가장 먼저 환경의 관측 공간(observation_space)에 맞는 특징 추출기를 만든다.
   * 사용자가 policy_kwargs를 통해 별도의 features_extractor_class를 지정하지 않으면, 관측 공간의 형태(이미지, 벡터, 사전 등)에 따라 SB3의 기본 추출기(CombinedExtractor)가 사용된다.
   * 이 모듈의 역할은 고차원의 관측값(Observation)을 저차원의 특징 벡터(Feature Vector)로 변환하는 것이다. 
 * 액터 (actor) 신경망 생성
   * **정책(Policy)**을 근사하는 신경망으로, 어떤 상태(State)에서 어떤 행동(Action)을 할지 결정한다.
   * 구조:
     * features_extractor로부터 특징 벡터(Features)를 입력받는다.
     * 이 특징 벡터를 mlp_extractor (정책용 MLP)에 통과시켜 한 단계 더 추상화한다.
     * 마지막으로 action_net을 통과하여 행동 확률 분포(Squashed Gaussian)의 파라미터(평균, 표준편차)를 출력한다.
   * actor는 자신의 파라미터만을 학습하기 위한 별도의 Optimizer를 가진다.
 * 크리틱 (critic & critic_target) 신경망 생성
   * **행동 가치(Q-value)**를 근사하는 신경망으로, 특정 상태에서 특정 행동을 했을 때 얼마나 좋은지를 평가한다.
   * SAC는 Q값 추정의 안정성을 위해 2개의 크리틱 네트워크와 2개의 타겟 네트워크를 사용한다. (critic과 critic_target)
   * 구조:
     * features_extractor로부터 특징 벡터(Features)를 입력받는다.
     * 입력받은 특징 벡터와 액터가 선택한 행동(Action) 벡터를 **결합(concatenate)**한다.
     * 결합된 벡터를 q_net (Q-함수용 신경망)에 통과시켜 최종 Q값(스칼라 값)을 출력한다.
   * critic 역시 학습을 위한 별도의 Optimizer를 가진다. critic_target은 학습되지 않고 critic의 가중치를 서서히 복사해온다 (Soft Update).
2. 옵티마이저 (Optimizer) 설정과 학습 흐름
SACPolicy 내부에서 옵티마이저는 액터와 크리틱에 대해 독립적으로 설정되지만, 학습 대상이 되는 파라미터의 범위에 매우 중요한 차이가 있다.
 * 액터 옵티마이저 (actor.optimizer)
   * 학습 대상: 오직 액터 신경망의 파라미터만 포함한다. (actor.parameters())
   * 목적: 크리틱이 계산한 Q값을 최대한 높이는 방향으로 정책(액터)을 업데이트한다. 즉, "더 좋은 평가를 받는 행동"을 하도록 액터를 학습시킨다.
 * 크리틱 옵티마이저 (critic.optimizer)
   * 학습 대상: 크리틱 신경망의 파라미터와 features_extractor의 파라미터를 모두 포함한다.
   * 목적: 벨만 방정식을 기반으로 Q값 추정의 정확도를 높이는 것. 즉, "현재 상태와 행동의 가치를 더 정확하게 예측"하도록 크리틱을 학습시킨다.
3. 왜 크리틱 옵티마이저가 특징 추출기를 제어하는가?
이것이 SAC 구현의 핵심적인 부분이다.
> 크리틱의 손실(Loss)은 "얼마나 Q값을 잘못 예측했는가"를 의미한다. 이 예측 오류는 Q값을 계산하는 크리틱 신경망 자체의 문제일 수도 있지만, 애초에 입력으로 들어온 상태 특징(State Feature)이 부실해서 발생할 수도 있다.

 * 만약 특징 추출기가 상태를 제대로 표현하지 못하는 벡터를 생성한다면, 크리틱은 그 부실한 정보를 바탕으로 Q값을 예측할 수밖에 없어 정확도가 떨어진다.
 * 따라서 크리틱의 손실을 줄이기 위해서는, 크리틱의 예측 능력뿐만 아니라 입력 재료를 만드는 특징 추출기의 표현 능력도 함께 개선해야 한다.
 * 이 때문에 크리틱의 손실(TD-Error)이 역전파(backpropagation)될 때, 그 그래디언트(gradient)는 크리틱 신경망을 거쳐 특징 추출기까지 흘러가 파라미터를 업데이트하게 된다.