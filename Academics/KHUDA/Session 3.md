- **사전 랭킹 모델 학습 (4.21):** 후보 생성 단계에서 추려진 수천 개의 후보를 수백 개로 줄이기 위해, 속도가 빠른 경량 모델을 학습시키는 과정임.
    
- **사전 랭킹 모델 평가 지표 (4.22):** 상위 N개의 후보 중 실제 사용자가 상호작용한 아이템이 얼마나 포함되었는지를 측정하는 재현율(recall)과 정밀도(precision)를 주로 사용해 평가함.
    
- **사전 랭킹 모델 최적화 (4.23):** 많은 후보를 빠르게 처리해야 하므로, 간단한 피처를 사용하거나 모델 양자화(quantization) 등의 기법을 통해 추론 속도를 최적화함.
    
- **랭킹 모델 추론 파이프라인 (4.24):** 사전 랭킹을 거친 수백 개의 후보에 대해, 더 복잡하고 정확한 헤비 랭킹 모델을 사용하여 최종 순위를 매기는 과정임.
    
- **텍스트 또는 ID 기반 피처 (4.25):** 사용자 ID, 아이템 ID와 같이 고유한 값이 많은 피처는 임베딩 테이블을 통해 저차원의 밀집 벡터(dense vector)로 변환하여 모델 입력값으로 사용함.
    
- **희소 기반 피처 (4.26):** 사용자가 클릭하지 않은 아이템 목록과 같이 값이 대부분 0인 희소한(sparse) 피처를 효율적으로 처리하는 기법을 의미함.
    
- **헤비 랭킹 모델 학습 (4.27):** 포인트별(pointwise), 페어별(pairwise), 리스트별(listwise) 등 다양한 접근 방식을 기반으로 손실 함수를 정의하여 모델을 학습함.
    
- **헤비 랭킹 모델 알고리즘 (4.28):** 더 정교한 순위 계산을 위해 그래디언트 부스팅 결정 트리(GBDT)나 심층 신경망(DNN)과 같이 복잡하고 표현력이 높은 모델을 사용함.
    
- **랭킹 모델 아키텍처 (4.29):** 피처 간의 복잡한 상호작용을 학습하기 위해 와이드 앤 딥(Wide & Deep), 딥FM(DeepFM), 트랜스포머(Transformer) 등 다양한 아키텍처를 활용함.
    
- **랭킹 모델 예측값 보정 (4.30):** 모델이 예측한 확률값이 실제 분포와 유사하도록 보정(calibration)하여, 예측값을 실제 비즈니스 의사결정에 신뢰도 높게 사용하도록 만듦.
    
- **랭킹 모델 평가 지표 (4.31):** 포인트별 지표(AUC, 로그 손실), 랭킹 지표(MRR, NDCG), 그리고 비즈니스 및 인프라 지표(클릭률, 지연 시간) 등을 종합적으로 사용하여 모델 성능을 다각도로 평가함.
    
- **다중 작업 모델과 개별 모델 (4.32):** 여러 비즈니스 목표(클릭, 구매 등)를 달성하기 위해, 각 목표별로 개별 모델을 만들거나 하나의 모델이 여러 목표를 동시에 학습하는 다중 작업 모델을 구축함.
    
- **모델 서빙 시스템 (4.33):** 피처 저장소에서 피처를 가져와 후보 생성, 사전 랭킹, 랭킹 모델을 순차적으로 거쳐 사용자에게 최종 추천 결과를 제공하는 전체 시스템을 의미함.
    
- **개선 (4.34):** 새로운 피처를 발굴하거나, 모델 아키텍처를 변경하고, 하이퍼파라미터를 튜닝하는 등 지속적인 실험을 통해 모델 성능을 개선함.
    
- **모델 업데이트 (4.35):** 변화하는 사용자 행동과 아이템 트렌드를 반영하기 위해 새로운 데이터로 모델을 주기적으로 다시 학습시킴.
    
- **온라인 실험 (4.36):** A/B 테스트를 통해 새로운 모델을 일부 사용자 그룹에 먼저 적용하여, 실제 서비스 환경에서의 성능을 기존 모델과 비교 평가함.
    
- **모델 로드 (4.37):** 학습이 완료된 모델 파일을 서빙 시스템에 배포하여, 실시간 예측 요청을 처리할 수 있도록 준비하는 과정임.
    
- **모델 실험 고려 사항 (4.38):** 실험 결과의 신뢰도를 높이기 위해 실험 기간, 트래픽 규모, 계절적 요인 등 다양한 변수를 신중하게 고려해야 함.
    
- **오프라인 평가 지표 (4.39):** 온라인 실험 이전에 과거 데이터를 이용해 모델 성능을 가늠하는 지표로, 실제 온라인 환경에서의 결과와는 차이가 발생할 수 있음을 인지해야 함.
    
- **온라인 성능 저하 (4.40):** 오프라인 테스트에서는 성능이 좋았던 모델이 실제 온라인 환경에서는 성능이 떨어지는 현상으로, 학습 데이터와 서빙 데이터의 불일치(training-serving skew) 등이 주요 원인임.
    

---

### 기술 질문 대비 핵심 정리

다음은 주어진 질문에 대해 답변할 때 언급해야 할 주요 용어와 내용입니다.

#### #22. 사전 랭킹 모델에 적합한 알고리즘과 그 발전 과정 (투-타워 → 경량 DNN)

사전 랭킹은 속도와 정확성의 균형이 중요합니다. 이 단계의 모델은 후보 생성에서 넘어온 수천 개의 후보를 수백 개로 빠르게 줄여주는 역할을 합니다.

- **언급할 용어 및 내용:**
    
    - **투-타워(Two-Tower) 모델:** 사용자 정보로 '사용자 타워'를, 아이템 정보로 '아이템 타워'를 각각 독립적으로 구성하여 임베딩을 생성하는 모델입니다. 두 타워에서 나온 임베딩의 내적(dot product)을 통해 유사도를 계산하며, 구조가 간단하고 각 타워의 임베딩을 미리 계산해 둘 수 있어 추론 속도가 매우 빠릅니다. 주로 후보 생성에 쓰이지만 사전 랭킹에도 활용 가능합니다.
        
    - **경량 DNN(Lightweight DNN):** 투-타워 모델보다 더 복잡한 피처 상호작용을 학습하면서도, 최종 랭킹 모델보다는 훨씬 가벼운 모델입니다. 은닉층의 수나 뉴런의 수를 줄인 간단한 DNN 구조를 통해 속도와 성능의 절충점을 찾습니다. 이는 투-타워 모델의 표현력 한계를 보완하고 더 정교한 필터링을 수행하기 위한 발전된 형태라고 할 수 있습니다.
        

#### #23. 많은 후보를 사전 랭킹할 때의 모델 최적화 방법

수천 개의 후보를 실시간으로 처리해야 하므로, 사전 랭킹 모델의 추론 속도 최적화는 매우 중요합니다.

- **언급할 용어 및 내용:**
    
    - **간단한 피처 사용:** 복잡한 연산이 필요한 피처 대신, 미리 계산되어 있거나 간단히 조회할 수 있는 정적인 피처 위주로 사용해 지연 시간을 줄입니다.
        
    - **모델 경량화:** 모델의 복잡도를 낮추기 위해 은닉층의 개수나 뉴런의 수를 줄여 전체 연산량을 감소시킵니다.
        
    - **양자화(Quantization):** 모델의 가중치 데이터 타입을 32비트 부동소수점에서 16비트나 8비트 정수형으로 변환합니다. 이는 모델의 크기를 줄이고 CPU/GPU의 연산 속도를 향상시킵니다.
        
    - **증류(Distillation):** 크고 복잡한 '교사 모델'(예: 헤비 랭킹 모델)의 예측 결과를 정답으로 삼아, 작고 빠른 '학생 모델'(사전 랭킹 모델)을 학습시키는 방법입니다. 교사 모델의 성능은 최대한 유지하면서 모델의 크기와 속도를 개선할 수 있습니다.
        

#### #25. 텍스트 또는 ID 기반 피처의 사용 방법

사용자 ID나 아이템 ID, 검색어와 같은 피처는 고유한 값의 개수가 매우 많아 직접 모델의 입력으로 사용하기 어렵습니다.

- **언급할 용어 및 내용:**
    
    - **고차원 범주형 피처:** 사용자 ID처럼 고유한 값의 종류가 수백만 개에 달할 수 있는 피처를 의미합니다. 이를 원-핫 인코딩으로 변환하면 벡터의 차원이 너무 커져 비효율적입니다.
        
    - **임베딩(Embedding):** 이러한 고차원의 희소(sparse) 피처를 저차원의 밀집(dense) 벡터로 변환하는 기법입니다.
        
    - **임베딩 테이블(Embedding Table):** 모든 ID 값에 대해 각각의 임베딩 벡터를 미리 학습하여 저장해 둔 일종의 조회 테이블입니다. 모델은 특정 ID 값이 입력되면 이 테이블에서 해당 ID에 매핑된 임베딩 벡터를 찾아와 입력으로 사용합니다. 텍스트의 경우, 단어나 문장 자체를 임베딩하여 하나의 벡터로 표현합니다.
        

#### #31. 랭킹 모델의 평가 지표

랭킹 모델은 하나의 지표가 아닌, 여러 관점의 지표를 종합적으로 사용하여 평가해야 합니다.

- **언급할 용어 및 내용:**
    
    - **포인트별 지표(Pointwise Metrics):** 각 아이템이 '정답'일 확률을 얼마나 잘 예측하는지, 개별 아이템 단위로 평가합니다.
        
        - **AUC:** 모델이 긍정 샘플을 부정 샘플보다 높은 점수로 예측할 확률을 나타내는 지표입니다.
            
        - **로그 손실(Log Loss):** 모델이 예측한 확률이 실제 값과 얼마나 차이 나는지를 측정합니다.
            
    - **랭킹 지표(Listwise Metrics):** 추천 목록 전체의 '순서' 품질을 평가합니다.
        
        - **MRR(Mean Reciprocal Rank):** 사용자가 상호작용한 첫 아이템이 목록 몇 번째에 나타나는지를 측정하여, 상위 순위의 정확도를 강조하는 지표입니다.
            
        - **NDCG(Normalized Discounted Cumulative Gain):** 순위가 높을수록 가중치를 부여하며, 아이템별 관련성의 등급까지 고려하여 목록의 전반적인 품질을 평가합니다.
            
    - **인프라 및 비즈니스 지표:** 실제 서비스 환경에서의 효율성과 비즈니스 기여도를 평가합니다.
        
        - **지연 시간(Latency):** 요청부터 응답까지 걸리는 시간으로, 사용자 경험에 직접적인 영향을 줍니다.
            
        - **클릭률(CTR), 전환율(CVR):** 모델이 비즈니스 목표에 얼마나 기여하는지를 측정하는 핵심 지표입니다.
            

#### #32. 여러 목표를 가진 모델의 구축 방법 (다중 작업 모델 vs. 개별 모델)

하나의 추천 시스템이 클릭, 구매, 좋아요 등 여러 사용자 행동을 동시에 유도해야 할 때, 두 가지 접근법을 고려할 수 있습니다.

- **언급할 용어 및 내용:**
    
    - **개별 모델(Individual Models):** 클릭 예측 모델, 구매 예측 모델 등 각 목표(objective)를 위한 모델을 별개로 학습시키는 방식입니다.
        
        - **장점:** 구현이 직관적이고, 각 모델을 독립적으로 최적화하기 쉽습니다.
            
        - **단점:** 모델의 수만큼 학습과 서빙 리소스가 필요해 비용이 많이 들고, 목표 간의 유용한 정보를 공유하지 못합니다.
            
    - **다중 작업 모델(Multi-task Model):** 하나의 모델이 여러 개의 목표를 동시에 학습하도록 설계하는 방식입니다.
        
        - **구조:** 모델의 하단부(Shared Bottom)는 모든 목표가 공유하고, 상단부는 각 목표를 위한 별도의 헤드(Head) 또는 타워(Tower)로 분리되는 구조가 일반적입니다.
            
        - **장점:** 여러 작업이 파라미터를 공유하므로 학습 및 서빙이 효율적입니다. 한 작업의 학습 데이터가 다른 작업에 긍정적 영향을 주어(전이 학습), 전반적인 성능이 향상될 수 있습니다.
            
        - **단점:** 서로 다른 목표들의 손실(loss)을 동시에 최적화하는 것이 까다롭고, 작업 간의 상충 관계로 인해 오히려 성능이 저하될 수도 있습니다.