### 1. 데이터셋 준비 및 처리

모델 학습을 위해 데이터를 수집하고 가공하는 단계와 관련된 용어임.

- **데이터 분할 (Data Splitting)**: 전체 데이터를 여러 세트로 나누는 과정임.
    
    - **학습/검증/테스트 셋 (Training/Validation/Test Sets)**: 모델 학습, 하이퍼파라미터 튜닝, 최종 성능 평가를 위해 데이터를 각각 분리한 것임.
        
    - **교차 검증 (Cross-validation)**: 학습과 검증 세트를 번갈아 나누어 모델 성능을 여러 번 평가하는 기법임.
        
- **학습 유형 (Learning Types)**
    
    - **지도 학습 (supervised learning)**: 정답(레이블)이 있는 데이터를 사용해 모델을 학습시키는 방식임.
        
- **샘플링 기법 (Sampling Techniques)**
    
    - **무작위 샘플링 (random sampling)**: 데이터셋에서 무작위로 데이터를 추출하는 기법임.
        
    - **계층화 샘플링 (stratified sampling)**: 데이터의 원래 분포를 유지하며 각 계층에서 데이터를 추출하는 기법임.
        
- **데이터 문제 및 해결 (Data Issues & Solutions)**
    
    - **서빙 편향 (serving bias)**: 데이터 수집 방식 자체로 인해 발생하는 편향을 의미함.
        
    - **무작위 누락 (Missing at Random, MAR)**: 데이터 누락이 특정 변수와 관련은 있지만, 누락된 값 자체와는 관련이 없는 상태임.
        
    - **레이블 불균형 (Label Imbalance)**: 특정 클래스의 데이터가 다른 클래스에 비해 현저히 많거나 적은 문제임.
        
        - **해결책**
            
            - **리샘플링 (Resampling)**: 데이터 수를 조절하는 기법임.
                
                - **오버샘플링 (Oversampling)**: 소수 클래스의 데이터를 복제하여 수를 늘리는 것임.
                    
                - **다운샘플링 (Downsampling)**: 다수 클래스의 데이터를 제거하여 수를 줄이는 것임.
                    
            - **합성 데이터 생성 (Synthetic Data Generation)**: 소수 클래스의 샘플을 인공적으로 생성하는 기법임.
                
                - **SMOTE**: 대표적인 합성 데이터 생성 알고리즘임.
                    
            - **비용 민감 학습 (Cost-sensitive learning)**: 학습 시 손실 함수에서 각 클래스에 다른 가중치를 부여하는 것임.
                
    - **누락된 레이블 (Missing Labels)**: 데이터에 레이블이 없는 경우를 처리하는 문제임.
        
        - **해결책**
            
            - **유도 (induction)**: 레이블이 있는 데이터만으로 모델을 학습시켜 누락된 레이블을 예측하는 것임.
                
            - **추론 (transduction)**: 레이블이 없는 데이터까지 학습에 활용하여 누락된 레이블을 예측하는 기법임.
                
                - **레이블 전파 알고리즘 (Label Propagation Algorithm, LPA)**: 그래프 구조에서 레이블 정보를 전파하는 준지도 학습 알고리즘임.
                    
                - **매니폴드 학습 (manifold learning)**: 고차원 데이터의 본질적인 저차원 구조(매니폴드)를 발견하여 학습하는 기법임.
                    

---

### 2. 피처 엔지니어링

데이터를 모델이 학습할 수 있는 특성(피처)으로 만들고 관리하는 단계와 관련된 용어임.

- **피처 유형 (Feature Types)**
    
    - **수치형 피처 (Numerical Features)**: 연속적이거나 이산적인 숫자 값임.
        
    - **범주형 피처 (Categorical Features)**: 특정 범주로 표현되는 값임.
        
        - **처리 기법**
            
            - **원-핫 인코딩 (one-hot encoding)**: 각 범주를 벡터의 한 차원으로 표현하는 것임.
                
            - **임베딩 (embedding)**: 고차원 범주형 피처를 저차원의 연속적인 벡터로 변환하는 것임.
                
            - **해싱 트릭 (hashing trick)**: 해시 함수를 이용해 피처를 더 작은 공간으로 압축하는 기법임.
                
    - **이진 피처 (binary features)**: 참/거짓 또는 예/아니요 같이 두 가지 값만 갖는 피처임.
        
- **피처 선택 (Feature Selection)**
    
    - **내재적 선택 (intrinsic selection)**: 모델링 과정 자체에 피처 선택 기능이 포함된 방식임.
        
    - **상관관계 (correlation)**
        
        - **공선성 (collinearity)**: 독립 변수들 간에 강한 상관관계가 나타나는 현상임.
            
- **피처 처리 (Feature Processing)**
    
    - **어휘에 포함되지 않은 값 (out-of-vocabulary, OOV)**: 모델이 학습 시 보지 못했던 새로운 단어나 ID 값을 의미함.
        
    - **증분적으로 (incrementally)**: 기존 모델을 다시 학습시키지 않고 새로운 데이터에 점진적으로 대응하는 방식임.
        
- **누락된 피처값 처리**
    
    - **다중 대치 (Multiple Imputation)**: 추정 분포에서 여러 값을 샘플링하여 복수의 완전한 데이터셋을 만드는 기법임.
        
        - **MICE (Multiple Imputation with Chained Equations)**: 대표적인 다중 대치 알고리즘임.
            

---

### 3. 모델링

데이터와 문제에 맞는 알고리즘을 선택하여 모델을 구축하는 단계와 관련된 용어임.

- **주요 알고리즘 (Major Algorithms)**
    
    - **로지스틱 회귀 (Logistic Regression)**: 이벤트 발생 확률을 모델링하는 분류 알고리즘임.
        
        - **로지스틱/시그모이드 함수 (Logistic/Sigmoid function)**: 입력값을 0과 1 사이의 확률값으로 변환하는 함수임.
            
        - **소프트맥스 함수 (Softmax function)**: 다중 클래스 분류에서 각 클래스에 대한 확률의 합이 1이 되도록 변환하는 함수임.
            
        - **준뉴턴 방법 (Quasi-Newton method)**: 2차 도함수(헤시안 행렬)를 근사하여 최적화하는 기법임.
            
    - **그래디언트 부스팅 결정 트리 (Gradient Boosted Decision Tree, GBDT)**: 이전 트리의 오차를 보완하는 방식으로 순차적으로 트리를 추가하는 앙상블 기법임.
        
        - **XGBoost (Xtreme Gradient Boosting)**: GBDT의 성능과 속도를 크게 개선한 인기 있는 구현체임.
            
        - **정보 이득 (information gain)**: 특정 피처로 데이터를 분할했을 때 불확실성이 얼마나 감소하는지를 나타내는 척도임.
            
    - **심층 신경망 (Deep Neural Network, DNN)**: 여러 개의 은닉층으로 구성되어 복잡한 비선형 관계를 학습할 수 있는 모델임.
        
        - **역전파 (backpropagation)**: 출력층의 오차를 입력층 방향으로 전파하며 각 뉴런의 가중치를 업데이트하는 학습 방식임.
            
        - **확률적 경사하강법 (Stochastic Gradient Descent, SGD)**: 전체 데이터가 아닌 일부 샘플을 사용해 가중치를 업데이트하여 학습 속도를 높이는 최적화 기법임.
            
- **비지도 학습 (Unsupervised Learning)**
    
    - **k-평균 (k-means)**: 데이터를 k개의 군집(클러스터)으로 나누는 알고리즘임.
        
    - **기댓값 최대화 (Expectation Maximization, EM)**: 잠재 변수가 있는 확률 모델에서 파라미터를 찾는 반복적인 알고리즘임.
        
    - **주성분 분석 (Principal Component Analysis, PCA)**: 데이터의 분산을 최대한 보존하는 새로운 축을 찾아 고차원 데이터를 저차원으로 축소하는 기법임.
        
    - **t-SNE (t-Distributed Stochastic Neighbor Embedding)**: 고차원 데이터의 유사성을 유지하며 저차원으로 시각화하는 알고리즘임.
        
    - **특잇값 분해 (Singular Value Decomposition, SVD)**: 행렬을 세 개의 다른 행렬의 곱으로 분해하는 기법임.
        
    - **오토인코더 (Autoencoder)**: 입력을 압축했다가 다시 원본으로 복원하며 데이터의 잠재 표현을 찾는 비지도 신경망임.
        
    - **변분 오토인코더 (Variational Autoencoder, VAE)**: 잠재 공간의 확률 분포를 학습하여 새로운 데이터를 생성할 수 있는 모델임.
        
        - **로그 가능도 (log-likelihood)**: 주어진 모델 하에서 특정 데이터가 관찰될 확률에 로그를 취한 값임.
            
        - **하한 (ELBO - Evidence Lower Bound)**: VAE에서 최적화하는 목적 함수로, 로그 가능도의 하한선임.
            
    - **제한 볼츠만 머신 (Restricted Boltzmann Machine, RBM)**: 가시층과 은닉층으로 구성된 생성 모델임.
        
    - **생성적 적대 신경망 (Generative Adversarial Network, GAN)**: 생성자와 판별자가 경쟁하며 실제 같은 데이터를 만드는 모델임.
        
- **준지도 학습 (Semi-supervised learning)**: 레이블이 있는 데이터와 없는 데이터를 모두 활용하여 모델을 학습시키는 방식임.
    
- **신경망 구성 요소 (Neural Network Components)**
    
    - **활성화 함수 (activation function)**: 뉴런의 입력 신호의 총합을 출력 신호로 변환하는 함수임.
        
        - **예시**: **시그모이드(Sigmoid)**, **소프트맥스(Softmax)**, **하이퍼탄젠트(Tanh)**, **ReLU(Rectified Linear Unit)**, **Leaky ReLU**, **ELU**, **Swish**
            
    - **경사 소실 (vanishing gradients)**: 신경망의 층이 깊어질수록 기울기가 점차 작아져 학습이 제대로 이루어지지 않는 문제임.
        
    - **부분 선형 (piecewise linear)**: 여러 개의 선형 함수 조각으로 이루어진 함수를 의미하며, **ReLU**가 대표적인 예임.
        
    - **0-중심 (zero-centered)**: 데이터의 평균이 0이 되도록 만든 상태로, 학습을 더 안정적으로 만드는 것임.
        

---

### 4. 모델 최적화 및 평가

모델의 성능을 극대화하고, 다양한 지표를 통해 객관적으로 평가하는 단계와 관련된 용어임.

- **과적합 방지 (Preventing Overfitting)**
    
    - **정규화 (regularization)**: 모델의 복잡도를 낮추기 위해 손실 함수에 페널티 항을 추가하는 기법임.
        
        - **L1 정규화 (Lasso regularization)**: 가중치의 절댓값을 페널티로 부여하며, 일부 가중치를 0으로 만들어 피처 선택 효과가 있는 것임.
            
        - **L2 정규화 (Ridge regularization)**: 가중치의 제곱을 페널티로 부여하며, 가중치 값을 전반적으로 작게 만드는 것임.
            
    - **드롭아웃 (dropout)**: 학습 과정에서 신경망의 일부 뉴런을 무작위로 비활성화하여 과적합을 방지하는 기법임.
        
    - **상호 적응 (co-adaptation)**: 뉴런들이 특정 뉴런의 오류를 보완하는 방향으로 함께 적응하여, 네트워크가 특정 패턴에만 과도하게 의존하게 되는 현상임.
        
- **최적화 (Optimization)**
    
    - **경사하강법 (gradient descent)**: 손실 함수의 기울기를 이용해 점진적으로 최솟값을 찾아가는 최적화 알고리즘임.
        
        - **최적화 알고리즘 예시**: **모멘텀(Momentum)**, **네스테로프 모멘텀(Nesterov Momentum)**, **AdaGrad**, **RMSProp**, **Adam**
            
    - **헤시안 행렬 (Hessian matrix)**: 다변수 함수의 모든 2차 편도함수를 행렬로 표현한 것으로, 함수의 곡률 정보를 담고 있음.
        
    - **안장점 (saddle point)**: 기울기가 0이지만 극값이 아닌 지점으로, 최적화 과정에서 학습이 정체될 수 있는 원인임.
        
- **하이퍼파라미터 튜닝 (Hyperparameter Tuning)**
    
    - **탐색 (exploration)**: 아직 시도해보지 않은 새로운 하이퍼파라미터 조합을 찾는 과정임.
        
    - **활용 (exploitation)**: 지금까지 가장 좋았던 하이퍼파라미터 조합 주변을 집중적으로 탐색하는 과정임.
        
    - **획득 함수 (acquisition function)**: 베이지안 최적화에서 다음 탐색 지점을 결정하는 함수임.
        
- **주요 손실 함수 (Loss Function)**
    
    - **회귀용**: **평균 제곱 오차(MSE)**, **평균 절대 오차(MAE)**, **후버 손실(Huber Loss)**, **포아송 손실(Poisson loss)**
        
    - **분류용**: **교차 엔트로피(Cross-Entropy)**, **힌지 손실(Hinge Loss)**, **쿨백-라이블러 발산(KLD)**
        
- **주요 평가 지표 (Evaluation Metrics)**
    
    - **분류용**:
        
        - **정확도(Accuracy)**: 전체 예측 중 올바르게 예측한 비율임.
            
        - **정밀도(Precision)**: 양성으로 예측한 것 중 실제 양성의 비율임.
            
        - **재현율(Recall)**: 실제 양성 중 양성으로 예측한 비율임.
            
        - **F1 점수(F1 Score)**: 정밀도와 재현율의 조화 평균임.
            
        - **정밀도@k (Precision@k)**: 상위 k개 결과 중 실제 양성의 비율임.
            
        - **AUC (Area Under the Curve)**: 모델의 판별 능력을 나타내는 지표로, **PR AUC**와 **ROC AUC**가 있음.
            
    - **회귀용**:
        
        - **MAE (Mean Absolute Error)**: 예측값과 실제값 차이의 절댓값 평균임.
            
        - **MSE (Mean Squared Error)**: 예측값과 실제값 차이의 제곱 평균임.
            
        - **RMSE (Root Mean Squared Error)**: MSE에 제곱근을 취한 값임.
            
        - **R-제곱 (R-squared)**: 모델이 데이터의 분산을 얼마나 설명하는지를 나타내는 지표임.
            
    - **랭킹용**:
        
        - **할인 누적 이득 (DCG)**: 검색 결과나 추천 목록의 순위를 평가하는 지표임.
            
        - **정규화 할인 누적 이득 (nDCG)**: DCG 값을 0과 1 사이로 정규화하여 다른 쿼리 간의 성능을 비교할 수 있게 만든 지표임.
            
        - **랭킹 학습 (learning-to-rank)**: 항목의 순위를 매기는 모델을 학습시키는 분야임.
        - 