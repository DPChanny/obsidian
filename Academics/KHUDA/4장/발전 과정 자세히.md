### 랭킹 모델 알고리즘 발전 과정

#### **1. 로지스틱 회귀 (Logistic Regression)**

단순성, 확장성, 해석 가능성으로 인해 오랫동안 포인트별 랭킹 모델에 널리 사용되었습니다.

- **적용 방식**:
    
    - 주로 ID나 키워드 같은 희소 피처(sparse features)를 다룹니다. 이 피처들은 원-핫 인코딩(one-hot encoding) 기법으로 이진화되어 모델에 입력됩니다.
        
    - 모델의
        
        **기억(memorization) 능력**을 높이기 위해, 여러 희소 피처를 조합하는 **피처 교차(feature crossing)**를 수동으로 생성하여 사용하는 경우가 많습니다. 예를 들어, '사용자 국가'와 '콘텐츠 카테고리'를 교차하여 '미국 사용자의 스포츠 콘텐츠 선호도'와 같은 세분화된 패턴을 학습합니다.
        
- **한계**: 피처 교차를 수동으로 엔지니어링해야 하는 부담이 크고, 데이터의 복잡한 패턴을 포착하기에는 모델 용량이 제한적이었습니다.
    

#### **2. 그래디언트 부스팅 (Gradient Boosting)**

피처 간의 비선형 상호 작용을 포착하고 누락된 값을 처리하는 뛰어난 성능으로 인해 인기를 얻었습니다.

- **적용 방식**:
    
    - XGBoost와 같은 알고리즘은 의사 결정 트리를 기본 모델로 사용합니다.
        
    - 모델은 이전 트리들이 만든 예측 오류를 다음 트리가 보완하는 방식으로 순차적으로 학습을 진행하며, 미분 가능한 손실 함수를 최적화하여 예측 오류를 최소화합니다.
        
- **장점**: 많은 실제 애플리케이션에서 로지스틱 회귀와 같은 기존 알고리즘보다 뛰어난 성능을 보였습니다.
    

#### **3. 하이브리드 (트리 + 로지스틱 회귀)**

부스팅 트리와 로지스틱 회귀의 장점을 결합한 방식입니다.

- **적용 방식**:
    
    1. 먼저, 그래디언트 부스팅 트리 모델을 학습시켜 피처 간의 복잡한 비선형 관계를 포착합니다.
        
    2. 학습된 트리의 각 리프 노드(leaf node)까지의 경로를 하나의 새로운 이진화된 파생 피처로 만듭니다. 예를 들어, 트리의 특정 경로를 따라 예측이 이루어졌다면 해당 경로를 나타내는 피처는 1, 나머지는 0이 됩니다.
        
    3. 이렇게 생성된 새로운 피처들을 로지스틱 회귀 모델에 입력하여 최종 예측을 수행합니다.
        
- **효과**: 이 방식은 링크드인, 페이스북 등에서 성공적으로 구현되었습니다.
    

#### **4. 심층 신경망 (DNN)**

방대한 데이터를 성능 저하 없이 처리하고, 특정 문제에 맞게 유연하게 조정할 수 있어 다른 ML 알고리즘을 능가하며 가장 널리 사용되는 모델이 되었습니다.

- **적용 방식**:
    
    - 가장 큰 변화는 수동 피처 엔지니어링의 부담을 크게 줄인 것입니다.
        
    - DNN은 각 범주형 피처에 대해 저차원의 밀집 임베딩 벡터(dense embedding vector)를 학습합니다. 이 임베딩 벡터들이 피처 간의 의미론적 관계를 내포하게 되어 모델이 스스로 패턴을 학습합니다.
        
- **한계**: 특히 틈새 아이템과 같이 상호 작용이 드문 희소 피처 간의 관계를 효과적으로 학습하는 데는 어려움이 있었습니다.
    

---

### 주요 랭킹 모델 아키텍처

#### **1. 와이드 앤 딥 (Wide & Deep)**

DNN의 한계를 극복하기 위해 구글이 도입한 아키텍처로, 기억 효과와 일반화 능력을 단일 모델로 결합했습니다.

- **구조 및 역할**:
    
    - **Wide 컴포넌트**: 선형 모델(로지스틱 회귀) 부분으로, 희소 피처와 교차 피처를 직접 사용하여 특정 규칙이나 패턴을 암기하는 **기억(memorization)** 역할을 수행합니다.
        
    - **Deep 컴포넌트**: DNN 부분으로, 피처들의 임베딩을 입력받아 이전에 보지 못한 피처 조합에 대해서도 예측할 수 있는 **일반화(generalization)** 능력을 담당합니다.
        

#### **2. 교차 네트워크 (DCN, DeepFM)**

와이드 앤 딥 이후, 피처 간 상호 작용을 더 효율적이고 명시적으로 모델링하기 위해 등장한 기법들입니다.

- **DeepFM (Deep Factorization Machine)**: 와이드 앤 딥의 Wide 부분을 인수 분해 기계(Factorization Machine)로 대체한 구조입니다. FM의 내적 유닛(inner product unit)이 피처들의 2차 상호 작용을 효과적으로 포착합니다.
    
- **DCN (Deep & Cross Network)**: 명시적인 교차 레이어(cross layer)를 도입하여 피처 상호 작용을 학습합니다. 각 교차 레이어는 이전 레이어의 출력과 초기 입력 임베딩 간의 상호 작용 함수로 구성되어 더 높은 차수의 복잡한 상호 작용까지 학습할 수 있습니다.
    

#### **3. 다중 작업 학습 (MTL) - MMoE**

클릭, 좋아요, 구독 등 여러 목표를 동시에 최적화해야 할 때 사용되는 아키텍처입니다.

- **MMoE (Multi-gate Mixture-of-Experts) 적용 방식**:
    
    - 여러 개의 하위 모델, 즉
        
        **전문가(Experts)** 네트워크를 둡니다. 이 전문가 네트워크들은 모든 작업이 공유하는 지식을 학습합니다.
        
    - 각 작업(task)마다 별도의
        
        **게이트(Gate)** 네트워크를 둡니다. 이 게이트는 해당 작업에 가장 유용한 전문가들의 결과물을 선택하고 조합하는 가중치를 학습합니다.
        
    - 이를 통해 작업 간 지식을 공유하면서도 각 작업에 최적화된 예측을 수행하여 전반적인 모델 성능을 향상시킵니다.
        

#### **4. 어텐션 및 트랜스포머**

주로 사용자 행동 이력과 같은 시퀀스 데이터에서 피처 간의 장거리 종속성을 포착하기 위해 도입된 강력한 기법입니다.

- **어텐션 메커니즘 적용 방식**:
    
    1. 어텐션은 **쿼리(Query), 키(Key), 밸류(Value)**라는 세 가지 요소로 구성됩니다. 예를 들어, 사용자의 현재 상호 작용(쿼리)과 과거 상호 작용 이력(키, 밸류)의 관계를 모델링할 수 있습니다.
        
    2. 쿼리 벡터와 모든 키 벡터 간의 유사도 점수를 계산합니다 (주로 내적 사용).
        
    3. 이 점수에 소프트맥스 함수를 적용하여 각 키(과거 행동)가 현재 쿼리와 얼마나 관련이 있는지 확률 분포(어텐션 가중치)로 변환합니다.
        
    4. 이 가중치를 밸류 벡터에 곱하여 가중 합을 구합니다. 이를 통해 현재 상황과 가장 관련성이 높은 과거 정보에 집중한 결과값을 생성합니다.
        
- **멀티헤드 어텐션**: 이러한 어텐션 과정을 여러 개의 "헤드"에서 병렬로 수행하여, 입력 요소 간의 다양한 측면의 상호 작용을 동시에 포착합니다.