### 1. 플랫 스케일링 (Platt Scaling)

플랫 스케일링은 복잡한 랭킹 모델(예: DNN)이 내놓은 예측 점수를 더 신뢰할 수 있는 확률로 변환하기 위해, 간단한

**로지스틱 회귀 모델을 한 번 더 사용**하는 기법입니다.

- **핵심 아이디어**: 랭킹 모델의 예측 점수와 실제 확률 사이의 관계가 왜곡되어 있을 때, 이 왜곡이 로지스틱(시그모이드) 함수 형태를 따른다고 가정합니다.
    
- **작동 방식**:
    
    1. **보정 데이터셋 준비**: 학습에 사용되지 않은 별도의 데이터셋을 준비합니다.
        
    2. **점수 예측**: 기존 랭킹 모델을 사용하여 이 보정 데이터셋의 예측 점수(0과 1 사이가 아닌 raw score)를 계산합니다.
        
    3. **로지스틱 회귀 학습**: 예측된 점수를 입력 피처(X)로, 실제 레이블(Y, 예: 클릭 여부 0 또는 1)을 정답으로 하여 로지스틱 회귀 모델을 학습시킵니다.
        
    4. **최종 확률**: 이 학습된 로지스틱 회귀 모델이 바로 '보정 레이어'가 됩니다. 앞으로 랭킹 모델이 새로운 예측 점수를 내놓으면, 이 점수를 보정 레이어(로지스틱 회귀 모델)에 통과시켜 최종적으로 보정된 확률을 얻습니다.
        
- **장점**: 구현이 간단하고 계산이 빠릅니다.
    
- **단점**: 모델 점수와 실제 확률 간의 관계가 반드시 로지스틱 함수 형태를 따른다는 보장이 없어, 이 가정이 맞지 않으면 보정 효과가 떨어질 수 있습니다.
    

### 2. 등장성 회귀 (Isotonic Regression)

등장성 회귀는 플랫 스케일링과 달리 특정 분포를 가정하지 않는

**비모수적(non-parametric) 방법**입니다.

- **핵심 아이디어**: 모델의 예측 점수가 높을수록, 보정된 확률도 반드시 같거나 높아져야 한다는 단조 증가(monotonically increasing) 제약 조건만 가지고 최적의 보정 함수를 찾아냅니다.
    
- **작동 방식**:
    
    1. **보정 데이터셋 준비**: 플랫 스케일링과 동일하게 별도의 데이터셋을 사용합니다.
        
    2. **계단 함수(Piecewise-constant function) 학습**: 등장성 회귀는 예측 점수들을 정렬한 뒤, 여러 구간으로 나누어 각 구간에 동일한 보정 값을 할당하는 **계단 형태의 함수**를 학습합니다.
        
    3. **오차 최소화**: 각 구간(계단의 각 발판)의 높이는, 해당 구간에 속한 데이터들의 **실제 값과 예측 값 간의 오차 제곱합을 최소화**하는 방향으로 결정됩니다.
        
    4. **최종 확률**: 이렇게 만들어진 계단 함수가 보정 레이어가 되어, 새로운 예측 점수가 들어오면 해당하는 구간의 보정된 확률 값을 반환합니다.
        
- **장점**: 특정 분포를 가정하지 않으므로 더 유연하고, 복잡한 왜곡 관계도 잘 잡아낼 수 있습니다.
    
- **단점**: 플랫 스케일링보다 더 많은 데이터가 필요할 수 있으며, 과적합(overfitting)의 위험이 있습니다.